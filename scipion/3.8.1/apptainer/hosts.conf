[localhost]
PARALLEL_COMMAND = mpirun -np %_(JOB_NODES)d -bynode %_(COMMAND)s
NAME = SLURM
MANDATORY = False
SUBMIT_COMMAND = sbatch %_(JOB_SCRIPT)s
CANCEL_COMMAND = scancel %_(JOB_ID)s
CHECK_COMMAND = squeue -h -j %_(JOB_ID)s
SUBMIT_TEMPLATE = #!/bin/bash
    ### Inherit all current environment variables
    #SBATCH --export=ALL
    ### Job name
    #SBATCH -J "%_(JOB_NAME)s - %_(SCIPION_PROTOCOL)s"
    ###SBATCH --comment "%_(SCIPION_PROJECT)s"
    ### Outputs
    #SBATCH -o %_(JOB_SCRIPT)s.out
    #SBATCH -e %_(JOB_SCRIPT)s.err
    #SBATCH --open-mode=append
    ### Partition (queue) name
    #SBATCH -p %_(JOB_QUEUE)s
    ### Specify time, number of nodes (tasks), cores and memory(MB) for your job
    #SBATCH --ntasks=%_(JOB_NODES)d --cpus-per-task=%_(JOB_THREADS)d --mem=65535 --gres=gpu:%_(GPU_COUNT)s

    ./launcher.sh %_(JOB_COMMAND)s

; Next variable is used to provide a regex to check if a job is finished on a queue system
JOB_DONE_REGEX = ""
QUEUES = {
    "debug-gpu": [["GPU_COUNT", "1", "Number of GPUs", "Select the number of GPUs if protocol has been set up to use them"],
              ["MemPerCPU", "1024", "Memory per CPU", "Minimum memory required per usable allocated CPU in megabytes"]],
    "short-gpu-small": [["GPU_COUNT", "1g.5gb:1", "Number of GPUs", "Select the number of GPUs if protocol has been set up to use them"],
                ["MemPerCPU", "1024", "Memory per CPU", "Minimum memory required per usable allocated CPU in megabytes"]],
    "short-gpu-big": [["GPU_COUNT", "a100:1", "Number of GPUs", "Select the number of GPUs if protocol has been set up to use them"],
                ["MemPerCPU", "1024", "Memory per CPU", "Minimum memory required per usable allocated CPU in megabytes"]],
    "long-gpu-small": [["GPU_COUNT", "1g.5gb:1", "Number of GPUs", "Select the number of GPUs if protocol has been set up to use them"],
                ["MemPerCPU", "1024", "Memory per CPU", "Minimum memory required per usable allocated CPU in megabytes"]],
    "long-gpu-big": [["GPU_COUNT", "a100:1", "Number of GPUs", "Select the number of GPUs if protocol has been set up to use them"],
               ["MemPerCPU", "1024", "Memory per CPU", "Minimum memory required per usable allocated CPU in megabytes"]],
    "debug-cpu": [["GPU_COUNT", "0", "Number of GPUs", "Select the number of GPUs if protocol has been set up to use them"],
               ["MemPerCPU", "1024", "Memory per CPU", "Minimum memory required per usable allocated CPU in megabytes"]],
    "short-cpu": [["GPU_COUNT", "0", "Number of GPUs", "Select the number of GPUs if protocol has been set up to use them"],
               ["MemPerCPU", "1024", "Memory per CPU", "Minimum memory required per usable allocated CPU in megabytes"]],
    "long-cpu": [["GPU_COUNT", "0", "Number of GPUs", "Select the number of GPUs if protocol has been set up to use them"],
               ["MemPerCPU", "1024", "Memory per CPU", "Minimum memory required per usable allocated CPU in megabytes"]],
    "bigmem-gpu-big": [["GPU_COUNT", "a100:1", "Number of GPUs", "Select the number of GPUs if protocol has been set up to use them"],
                ["MemPerCPU", "1024", "Memory per CPU", "Minimum memory required per usable allocated CPU in megabytes"]],
    "highmem-gpu-big": [["GPU_COUNT", "a100:1", "Number of GPUs", "Select the number of GPUs if protocol has been set up to use them"],
                ["MemPerCPU", "1024", "Memory per CPU", "Minimum memory required per usable allocated CPU in megabytes"]],
    "bigmem-gpu-small": [["GPU_COUNT", "1g.5gb:1", "Number of GPUs", "Select the number of GPUs if protocol has been set up to use them"],
                ["MemPerCPU", "1024", "Memory per CPU", "Minimum memory required per usable allocated CPU in megabytes"]],
    "highmem-gpu-small": [["GPU_COUNT", "1g.5gb:1", "Number of GPUs", "Select the number of GPUs if protocol has been set up to use them"],
                ["MemPerCPU", "1024", "Memory per CPU", "Minimum memory required per usable allocated CPU in megabytes"]]
}
