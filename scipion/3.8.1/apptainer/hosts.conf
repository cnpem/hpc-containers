[localhost]
PARALLEL_COMMAND = mpirun -np %_(JOB_NODES)d -bynode %_(COMMAND)s
NAME = SLURM
MANDATORY = False
SUBMIT_COMMAND = sbatch %_(JOB_SCRIPT)s
CANCEL_COMMAND = scancel %_(JOB_ID)s
CHECK_COMMAND = squeue -h -j %_(JOB_ID)s
SUBMIT_TEMPLATE = #!/bin/bash
    #SBATCH --export=ALL
    #SBATCH -J %_(JOB_NAME)s
    #SBATCH -o %_(JOB_LOGS)s.out
    #SBATCH -e %_(JOB_LOGS)s.err
    #SBATCH -p %_(JOB_QUEUE)s
    ##### SBATCH --time=%_(JOB_TIME)s:00:00 --ntasks=%_(JOB_NODES)d --cpus-per-task=%_(JOB_THREADS)d --mem=%_(JOB_MEMORY)s --gres=gpu:%_(GPU_COUNT)s
    #SBATCH --time=%_(JOB_TIME)s:00:00 --ntasks=%_(JOB_NODES)d --cpus-per-task=%_(JOB_THREADS)d --gres=gpu:%_(GPU_COUNT)s
    
    WORKDIR=$SLURM_SUBMIT_DIR

    export SCIPION_RUN_COMMAND_SCRIPT=/tmp/run_cmd.sh

    #################################
    ### Set environment variable to know running mode is non interactive
    export XMIPP_IN_QUEUE=1

    cd $WORKDIR
    # Make a copy of node file
    echo $SLURM_JOB_NODELIST > %_(JOB_NODEFILE)s
    # Calculate the number of processors allocated to this run.
    #NPROCS=$SLURM_NTASKS
    # Calculate the number of nodes allocated.
    NPROCS=$SLURM_NTASKS
    NNODES=$SLURM_JOB_NUM_NODES

    ### Display the job context
    echo Running on host `hostname`
    echo Time is `date`
    echo Working directory is `pwd`
    echo Using ${NPROCS} processors across ${NNODES} nodes
    echo NODE LIST - config:
    echo $SLURM_JOB_NODELIST
    echo CUDA_VISIBLE_DEVICES: $CUDA_VISIBLE_DEVICES
    echo HOME: $HOME
    echo SLURM_BASE: $SLURM_BASE
    echo SLURM_BIN: $SLURM_BIN
    echo SLURM_LIB: $SLURM_LIB
    echo SCIPION3_CONTAINER: $SCIPION3_CONTAINER
    #################################
    # echo '%_(JOB_COMMAND)s' >> /tmp/slurm-jobs.log
    # %_(JOB_COMMAND)s
    cat > /tmp/run_cmd.sh <<EOF
    scipion3 run %_(JOB_COMMAND)s
    EOF
    chmod +x /tmp/run_cmd.sh
    apptainer exec --nv --containall --bind /tmp --no-mount /etc/localtime --bind $HOME --env SCIPION_USER_DATA=$HOME/ScipionUserData --bind /mnt/beegfs --bind /mnt/beegfs_compat --bind /mnt/bgfs --bind /opt/ohpc/pub/apps/scipion/containers/scipion-3.8.1/hosts.conf:/opt/scipion/config/hosts.conf $SCIPION3_CONTAINER bash $SCIPION_RUN_COMMAND_SCRIPT
    find "$SLURM_SUBMIT_DIR" -type f -perm 0644 -exec chmod 0664 {} +

QUEUES = {
    "debug-gpu": [["GPU_COUNT", "1", "Number of GPUs", "Select the number of GPUs if protocol has been set up to use them"],
              ["MemPerCPU", "1024", "Memory per CPU", "Minimum memory required per usable allocated CPU in megabytes"]],
    "short-gpu-small": [["GPU_COUNT", "1g.5gb:1", "Number of GPUs", "Select the number of GPUs if protocol has been set up to use them"],
                ["MemPerCPU", "1024", "Memory per CPU", "Minimum memory required per usable allocated CPU in megabytes"]],
    "short-gpu-big": [["GPU_COUNT", "a100:1", "Number of GPUs", "Select the number of GPUs if protocol has been set up to use them"],
                ["MemPerCPU", "1024", "Memory per CPU", "Minimum memory required per usable allocated CPU in megabytes"]],
    "long-gpu-small": [["GPU_COUNT", "1g.5gb:1", "Number of GPUs", "Select the number of GPUs if protocol has been set up to use them"],
                ["MemPerCPU", "1024", "Memory per CPU", "Minimum memory required per usable allocated CPU in megabytes"]],
    "long-gpu-big": [["GPU_COUNT", "a100:1", "Number of GPUs", "Select the number of GPUs if protocol has been set up to use them"],
               ["MemPerCPU", "1024", "Memory per CPU", "Minimum memory required per usable allocated CPU in megabytes"]],
    "debug-cpu": [["GPU_COUNT", "0", "Number of GPUs", "Select the number of GPUs if protocol has been set up to use them"],
               ["MemPerCPU", "1024", "Memory per CPU", "Minimum memory required per usable allocated CPU in megabytes"]],
    "short-cpu": [["GPU_COUNT", "0", "Number of GPUs", "Select the number of GPUs if protocol has been set up to use them"],
               ["MemPerCPU", "1024", "Memory per CPU", "Minimum memory required per usable allocated CPU in megabytes"]],
    "long-cpu": [["GPU_COUNT", "0", "Number of GPUs", "Select the number of GPUs if protocol has been set up to use them"],
               ["MemPerCPU", "1024", "Memory per CPU", "Minimum memory required per usable allocated CPU in megabytes"]],
    "bigmem-gpu-big": [["GPU_COUNT", "a100:1", "Number of GPUs", "Select the number of GPUs if protocol has been set up to use them"],
                ["MemPerCPU", "1024", "Memory per CPU", "Minimum memory required per usable allocated CPU in megabytes"]],
    "highmem-gpu-big": [["GPU_COUNT", "a100:1", "Number of GPUs", "Select the number of GPUs if protocol has been set up to use them"],
                ["MemPerCPU", "1024", "Memory per CPU", "Minimum memory required per usable allocated CPU in megabytes"]],
    "bigmem-gpu-small": [["GPU_COUNT", "1g.5gb:1", "Number of GPUs", "Select the number of GPUs if protocol has been set up to use them"],
                ["MemPerCPU", "1024", "Memory per CPU", "Minimum memory required per usable allocated CPU in megabytes"]],
    "highmem-gpu-small": [["GPU_COUNT", "1g.5gb:1", "Number of GPUs", "Select the number of GPUs if protocol has been set up to use them"],
                ["MemPerCPU", "1024", "Memory per CPU", "Minimum memory required per usable allocated CPU in megabytes"]]}
